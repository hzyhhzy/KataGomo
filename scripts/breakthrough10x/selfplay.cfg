

maxVisits = 150
cheapSearchProb = 0.0  # Do cheap searches with this probaiblity
cheapSearchVisits = 80  # Number of visits for cheap search
reducedVisitsMin = 20  # Minimum number of visits (never reduce below this)


dataBoardLen = 10
bSizesX = 2,3,4,5,6,7,8,9,10
bSizeRelProbsX = 1,4,9,16,25,36,100,100,100
bSizesY = 4,5,6,7,8,9,10
bSizeRelProbsY = 1,4,9,16,60,100,100
allowRectangleProb = 0.80

scoringRules = AREA


cpuctExploration = 1.1
cpuctExplorationLog = 0.0

normalAsymmetricPlayoutProb = 0.5  # In regular games, play with unbalanced players with this probability
maxAsymmetricRatio = 4.0 # Max ratio to unbalance visits between players


komiMean = 0.0 # Specify explicit komi
komiStdev = 0.0  # Standard deviation of random variation to komi.
komiBigStdevProb = 0.0 # Probability of applying komiBigStdev
komiBigStdev = 12.0 # Standard deviation of random big variation to komi
komiBiggerStdevProb = 0.0 # Probability of applying komiBiggerStdev
komiBiggerStdev = 30.0 # Standard deviation of random bigger variation to komi

numNNServerThreadsPerModel = 2
gpuToUseThread0 = 0
gpuToUseThread1 = 1  
gpuToUseThread2 = 2  
gpuToUseThread3 = 3  
numGameThreads = 1100
nnMaxBatchSize = 512

# Logs------------------------------------------------------------------------------------

logSearchInfo = false
logMoves = false
logGamesEvery = 500
logToStdout = true

# Data writing-----------------------------------------------------------------------------------

# Spatial size of tensors of data, must match -pos-len in python/train.py and be at least as large as the
# largest boardsize in the data. If you train only on smaller board sizes, you can decrease this and
# pass in a smaller -pos-len to python/train.py (e.g. modify "-pos-len 19" in selfplay/train.sh script),
# to train faster, although it may be awkward if you ever want to increase it again later since data with
# different values cannot be shuffled together.

maxDataQueueSize = 50000
maxRowsPerTrainFile = 50000
maxRowsPerValFile = 50000
firstFileRandMinProp = 0.15
validationProp = 0.0

# Fancy game selfplay settings--------------------------------------------------------------------

# These take dominance - if a game is forked for any of these reasons, that will be the next game played.
# For forked games, randomization of rules and "initGamesWithPolicy" is disabled, komi is made fair with prob "forkCompensateKomiProb".
earlyForkGameProb = 0.00  # Fork to try alternative opening variety with this probability
earlyForkGameExpectedMoveProp = 0.025  # Fork roughly within the first (boardArea * this) many moves
forkGameProb = 0.0 # Fork to try alternative crazy move anywhere in game with this probability if not early forking
forkGameMinChoices = 3   # Choose from the best of at least this many random choices
earlyForkGameMaxChoices = 12  # Choose from the best of at most this many random choices
forkGameMaxChoices = 36  # Choose from the best of at most this many random choices
# Otherwise, with this probability, learn a bit more about the differing evaluation of seki in different rulesets.
sekiForkHackProb = 0.0

# Otherwise, play some proportion of games starting from SGF positions, with randomized rules (ignoring the sgf rules)
# On SGF positions, high temperature policy init is allowed
# startPosesProb = 0.0  # Play this proportion of games starting from SGF positions
# startPosesFromSgfDir = DIRECTORYPATH  # Load SGFs from this dir
# startPosesLoadProb = 1.0  # Only load each position from each SGF with this chance (save memory)
# startPosesTurnWeightLambda = 0  # 0 = equal weight  0.01 = decrease probability by 1% per turn  -0.01 = increase probability by 1% per turn.
# startPosesPolicyInitAreaProp = 0.0  # Same as policyInitAreaProp but for SGF positions

# Otherwise, play some proportion of games starting from hint positions (generated using "dataminesgfs" command), with randomized rules.
# On hint positions, "initGamesWithPolicy" does not apply.
# hintPosesProb = 0.0
# hintPosesDir = DIRECTORYPATH

# Otherwise we are playing a "normal" game, potentially with handicap stones, depending on "handicapProb", and
# potentially with komi randomization, depending on things like "komiStdev", and potentially with different
# board sizes, etc.

# Most of the remaining parameters here below apply regardless of the initialization, although a few of them
# vary depending on handicap vs normal game, and some are explicitly disabled (e.g. initGamesWithPolicy on hint positions).

initGamesWithPolicy = true  # Play the first few moves of a game high-temperaturely from policy
policyInitAreaTemperature=1.8
policyInitAreaProp = 0.15 # The avg number of moves to play
policyInitAvgMoveNum = 10
compensateAfterPolicyInitProb = 0.0 # Additionally make komi fair this often after the high-temperature moves.
sidePositionProb = 0.0  # With this probability, train on refuting bad alternative moves.

cheapSearchTargetWeight = 0.0  # Training weight for cheap search

reduceVisits = true  # Reduce visits when one side is winning
reduceVisitsThreshold = 0.9  # How winning a side needs to be (winrate)
reduceVisitsThresholdLookback = 3  # How many consecutive turns needed to be that winning
reducedVisitsWeight = 0.1  # Minimum training weight

handicapAsymmetricPlayoutProb = 0.0  # In handicap games, play with unbalanced players with this probablity
minAsymmetricCompensateKomiProb = 0.05 # Compensate komi with at least this probability for unbalanced players

policySurpriseDataWeight = 0.5  # This proportion of training weight should be concentrated on surprising moves
valueSurpriseDataWeight = 0.1   # This proportion of training weight should be concentrated on surprising position results

estimateLeadProb = 0.0 # Train lead, rather than just scoremean. Consumes a decent number of extra visits, can be quite slow using low visits to set too high.
switchNetsMidGame = false  # When a new neural net is loaded, switch to it immediately instead of waiting for new game
fancyKomiVarying = false # In non-compensated handicap and fork games, vary komi to better learn komi and large score differences that would never happen in even games.

# Match-----------------------------------------------------------------------------------

maxMovesPerGame = 160000

# Rules------------------------------------------------------------------------------------

#taxRules = NONE,NONE,SEKI,SEKI,ALL
#hasButtons = false,false,true


#komiAuto = True  # Automatically adjust komi to what the neural nets think are fair based on the empty board, but still apply komiStdev.


handicapProb = 0.00 # Probability of handicap game
handicapCompensateKomiProb = 0.50 # In handicap games, adjust komi to fair with this probability based on the handicap placement
forkCompensateKomiProb = 0.80     # For forks, adjust komi to fair with this probability based on the forked position
sgfCompensateKomiProb = 0.90      # For sgfs, adjust komi to fair with this probability based on the specific starting position

drawRandRadius = 0.0
noResultStdev = 0.0
noResultRandRadius = 0.0

# Search limits-----------------------------------------------------------------------------------

numSearchThreads = 1

# GPU Settings-------------------------------------------------------------------------------

nnCacheSizePowerOfTwo = 26
nnMutexPoolSizePowerOfTwo = 21
nnRandomize = true

# CUDA GPU settings--------------------------------------
# cudaDeviceToUse = 0 #use device 0 for all server threads (numNNServerThreadsPerModel) unless otherwise specified per-model or per-thread-per-model
# cudaDeviceToUseModel0 = 3 #use device 3 for model 0 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel1 = 2 #use device 2 for model 1 for all threads unless otherwise specified per-thread for this model
# cudaDeviceToUseModel0Thread0 = 3 #use device 3 for model 0, server thread 0
# cudaDeviceToUseModel0Thread1 = 2 #use device 2 for model 0, server thread 1

useFP16 = auto
useNHWC = auto

# Root move selection and biases------------------------------------------------------------------------------

chosenMoveTemperatureEarly = 0.75
chosenMoveTemperatureHalflife = 19
chosenMoveTemperature = 0.25
chosenMoveSubtract = 5
chosenMovePrune = 1

rootNoiseEnabled = true
rootDirichletNoiseTotalConcentration = 10.83
rootDirichletNoiseWeight = 0.25

rootDesiredPerChildVisitsCoeff = 2
rootNumSymmetriesToSample = 2

useLcbForSelection = true
lcbStdevs = 5.0
minVisitPropForLCB = 0.15

# Internal params------------------------------------------------------------------------------

winLossUtilityFactor = 1.0
staticScoreUtilityFactor = 0.00
dynamicScoreUtilityFactor = 0.0
dynamicScoreCenterZeroWeight = 0.25
dynamicScoreCenterScale = 0.50
noResultUtilityForWhite = 0.0
drawEquivalentWinsForWhite = 0.5

rootEndingBonusPoints = 0.0
rootPruneUselessMoves = true

rootPolicyTemperatureEarly = 1.6
rootPolicyTemperature = 1.1

fpuReductionMax = 0.2
rootFpuReductionMax = 0.0

numVirtualLossesPerThread = 1

# These parameters didn't exist historically during early KataGo runs
valueWeightExponent = 0.5
subtreeValueBiasFactor = 0.00
subtreeValueBiasWeightExponent = 0.8
useNonBuggyLcb = true
useGraphSearch = true
fpuParentWeightByVisitedPolicy = true
fpuParentWeightByVisitedPolicyPow = 2.0
